{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始时间：2020.12.15-16:02:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_i:1, episode_len:11\n",
      "batch_i:2, episode_len:12\n",
      "batch_i:3, episode_len:18\n",
      "batch_i:4, episode_len:14\n",
      "batch_i:5, episode_len:14\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-51a3388682c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[0mtime_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"开始时间：\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y.%m.%d-%H:%M:%S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m     \u001b[0mtraining_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m     \u001b[0mtime_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"结束时间：\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtime_end\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y.%m.%d-%H:%M:%S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-51a3388682c9>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m                 \u001b[1;31m#logging.debug(\"batch_i:{}, episode_len:{}\".format(i+1, self.episode_len))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m                     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"batch_i {} loss {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[1;31m#check the performance of the current model锛宎nd save the model params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-51a3388682c9>\u001b[0m in \u001b[0;36mpolicy_update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mold_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_value_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_value_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmcts_probs_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwinner_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_multiplier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m             \u001b[0mnew_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_value_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[0mkl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_probs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mold_probs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_probs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\AlphaZero_Gomoku\\policy_value_net_pytorch.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, state_batch, mcts_probs, winner_batch, lr)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m# calc policy entropy, for monitoring only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0mentropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_act_probs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlog_act_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_policy_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "An implementation of the training pipeline of AlphaZero for Gomoku\n",
    "\n",
    "@author: Junxiao Song\n",
    "\"\"\" \n",
    "\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle  #import cPickle as pickle\n",
    "from collections import defaultdict, deque\n",
    "from game import Board, Game\n",
    "#from policy_value_net import PolicyValueNet  # Theano and Lasagne\n",
    "from policy_value_net_pytorch import PolicyValueNet  # Pytorch\n",
    "from mcts_pure import MCTSPlayer as MCTS_Pure\n",
    "from mcts_alphaZero import MCTSPlayer\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "#save the log in the file \"loss_win.log\", filemode=\"w\"表示 每次覆盖log文件\n",
    "logging.basicConfig(filename=\"loss_win.log\",level=logging.DEBUG,format=\"%(message)s\",filemode=\"w\")\n",
    "\n",
    "\n",
    "class TrainPipeline():\n",
    "    def __init__(self, init_model=None):\n",
    "        # params of the board and the game\n",
    "        self.board_width = 10  #6 #10\n",
    "        self.board_height = 10 #6 #10\n",
    "        self.n_in_row = 5     #4 #5\n",
    "        self.board = Board(width=self.board_width, height=self.board_height, n_in_row=self.n_in_row)\n",
    "        self.game = Game(self.board)\n",
    "        # training params \n",
    "        self.learn_rate = 5e-3\n",
    "        self.lr_multiplier = 1.0  # adaptively adjust the learning rate based on KL\n",
    "        self.temp = 1.0 # the temperature param\n",
    "        self.n_playout = 400 # num of simulations for each move\n",
    "        self.c_puct = 5\n",
    "        self.buffer_size = 10000\n",
    "        self.batch_size = 512 # mini-batch size for training\n",
    "        self.data_buffer = deque(maxlen=self.buffer_size)        \n",
    "        self.play_batch_size = 1 \n",
    "        self.epochs = 5 # num of train_steps for each update\n",
    "        self.kl_targ = 0.025\n",
    "        self.check_freq = 50  #50\n",
    "        self.game_batch_num = 1500\n",
    "        self.best_win_ratio = 0.0\n",
    "        # num of simulations used for the pure mcts, which is used as the opponent to evaluate the trained policy\n",
    "        self.pure_mcts_playout_num = 1000  \n",
    "        if init_model:\n",
    "            # start training from an initial policy-value net\n",
    "            policy_param = pickle.load(open(init_model, 'rb')) \n",
    "            self.policy_value_net = PolicyValueNet(self.board_width, self.board_height, net_params = policy_param)\n",
    "        else:\n",
    "            # start training from a new policy-value net\n",
    "            self.policy_value_net = PolicyValueNet(self.board_width, self.board_height) \n",
    "        self.mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn, c_puct=self.c_puct, n_playout=self.n_playout, is_selfplay=1)\n",
    "\n",
    "    def get_equi_data(self, play_data):\n",
    "        \"\"\"\n",
    "        augment the data set by rotation and flipping\n",
    "        play_data: [(state, mcts_prob, winner_z), ..., ...]\"\"\"\n",
    "        extend_data = []\n",
    "        for state, mcts_porb, winner in play_data:\n",
    "            for i in [1,2,3,4]:\n",
    "                # rotate counterclockwise \n",
    "                equi_state = np.array([np.rot90(s,i) for s in state])\n",
    "                equi_mcts_prob = np.rot90(np.flipud(mcts_porb.reshape(self.board_height, self.board_width)), i)\n",
    "                extend_data.append((equi_state, np.flipud(equi_mcts_prob).flatten(), winner))\n",
    "                # flip horizontally\n",
    "                equi_state = np.array([np.fliplr(s) for s in equi_state])\n",
    "                equi_mcts_prob = np.fliplr(equi_mcts_prob)\n",
    "                extend_data.append((equi_state, np.flipud(equi_mcts_prob).flatten(), winner))\n",
    "        return extend_data\n",
    "                \n",
    "    def collect_selfplay_data(self, n_games=1):\n",
    "        \"\"\"collect self-play data for training\"\"\"\n",
    "        for i in range(n_games):\n",
    "            winner, play_data = self.game.start_self_play(self.mcts_player, temp=self.temp)\n",
    "            play_data_zip2list = list(play_data)  # add by haward\n",
    "            self.episode_len = len(play_data_zip2list)\n",
    "            # augment the data\n",
    "            play_data = self.get_equi_data(play_data_zip2list)\n",
    "            self.data_buffer.extend(play_data)\n",
    "                        \n",
    "    def policy_update(self):\n",
    "        \"\"\"update the policy-value net\"\"\"\n",
    "        mini_batch = random.sample(self.data_buffer, self.batch_size)\n",
    "        state_batch = [data[0] for data in mini_batch]\n",
    "        mcts_probs_batch = [data[1] for data in mini_batch]\n",
    "        winner_batch = [data[2] for data in mini_batch]            \n",
    "        old_probs, old_v = self.policy_value_net.policy_value(state_batch) \n",
    "        for i in range(self.epochs): \n",
    "            loss, entropy = self.policy_value_net.train_step(state_batch, mcts_probs_batch, winner_batch, self.learn_rate*self.lr_multiplier)\n",
    "            new_probs, new_v = self.policy_value_net.policy_value(state_batch)\n",
    "            kl = np.mean(np.sum(old_probs * (np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)), axis=1))  \n",
    "            if kl > self.kl_targ * 4:   # early stopping if D_KL diverges badly\n",
    "                break\n",
    "        # adaptively adjust the learning rate\n",
    "        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:\n",
    "            self.lr_multiplier /= 1.5\n",
    "        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\n",
    "            self.lr_multiplier *= 1.5\n",
    "            \n",
    "        explained_var_old =  1 - np.var(np.array(winner_batch) - old_v.flatten())/np.var(np.array(winner_batch))\n",
    "        explained_var_new = 1 - np.var(np.array(winner_batch) - new_v.flatten())/np.var(np.array(winner_batch))        \n",
    "        print(\"kl:{:.5f},lr_multiplier:{:.3f},loss:{},entropy:{},explained_var_old:{:.3f},explained_var_new:{:.3f}\".format(\n",
    "                kl, self.lr_multiplier, loss, entropy, explained_var_old, explained_var_new))\n",
    "        return loss, entropy\n",
    "        \n",
    "    def policy_evaluate(self, n_games=10,batch=0):\n",
    "        \"\"\"\n",
    "        Evaluate the trained policy by playing games against the pure MCTS player\n",
    "        Note: this is only for monitoring the progress of training\n",
    "        \"\"\"\n",
    "        current_mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn, c_puct=self.c_puct, n_playout=self.n_playout)\n",
    "        pure_mcts_player = MCTS_Pure(c_puct=5, n_playout=self.pure_mcts_playout_num)\n",
    "        win_cnt = defaultdict(int)\n",
    "        for i in range(n_games):\n",
    "            winner = self.game.start_play(current_mcts_player, pure_mcts_player, start_player=i%2, is_shown=0)\n",
    "            win_cnt[winner] += 1\n",
    "        win_ratio = 1.0*(win_cnt[1] + 0.5*win_cnt[-1])/n_games\n",
    "        print(\"batch_i:{}, num_playouts:{}, win: {}, lose: {}, tie:{}\".format(batch, self.pure_mcts_playout_num, win_cnt[1], win_cnt[2], win_cnt[-1]))\n",
    "        logging.debug(\"batch_i {} num_playouts {} win {} lose {} tie {}\".format(batch, self.pure_mcts_playout_num, win_cnt[1], win_cnt[2], win_cnt[-1]))\n",
    "        return win_ratio\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"run the training pipeline\"\"\"\n",
    "        try:\n",
    "            for i in range(self.game_batch_num):                \n",
    "                self.collect_selfplay_data(self.play_batch_size)\n",
    "                print(\"batch_i:{}, episode_len:{}\".format(i+1, self.episode_len))\n",
    "                #logging.debug(\"batch_i:{}, episode_len:{}\".format(i+1, self.episode_len))\n",
    "                if len(self.data_buffer) > self.batch_size:\n",
    "                    loss, entropy = self.policy_update()\n",
    "                    logging.debug(\"batch_i {} loss {}\".format(i+1, loss))\n",
    "                #check the performance of the current model锛宎nd save the model params\n",
    "                if (i+1) % self.check_freq == 0:\n",
    "                    print(\"current self-play batch: {}\".format(i+1))\n",
    "                    win_ratio = self.policy_evaluate(batch= i+1)\n",
    "                    net_params = self.policy_value_net.get_policy_param() # get model params\n",
    "                    pickle.dump(net_params, open('current_policy_8_8_5_new.model', 'wb'), pickle.HIGHEST_PROTOCOL) # save model param to file\n",
    "                    if win_ratio > self.best_win_ratio: \n",
    "                        print(\"New best policy!!!!!!!!\")\n",
    "                        self.best_win_ratio = win_ratio\n",
    "                        pickle.dump(net_params, open('best_policy_8_8_5_new.model', 'wb'), pickle.HIGHEST_PROTOCOL) # update the best_policy\n",
    "                        if self.best_win_ratio == 1.0 and self.pure_mcts_playout_num < 5000:\n",
    "                            self.pure_mcts_playout_num += 1000\n",
    "                            self.best_win_ratio = 0.0\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\n\\rquit')\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    training_pipeline = TrainPipeline()   # 从零训练 二选一\n",
    "#     training_pipeline = TrainPipeline(\"best_policy_8_8_5_new.model\")   # 增量训练 二选一\n",
    "    time_start = datetime.datetime.now()\n",
    "    print(\"开始时间：\" + time_start.strftime('%Y.%m.%d-%H:%M:%S'))\n",
    "    training_pipeline.run()\n",
    "    time_end = datetime.datetime.now()\n",
    "    print(\"结束时间：\" + time_end.strftime('%Y.%m.%d-%H:%M:%S'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
